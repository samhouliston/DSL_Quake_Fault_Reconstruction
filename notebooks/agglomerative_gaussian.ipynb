{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from sklearn import datasets\n",
    "from scipy.cluster.hierarchy import ward, fcluster\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.spatial import ConvexHull\n",
    "import open3d as o3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state=71"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a simple dataset\n",
    "\n",
    "n_samples = 500\n",
    "n = 4\n",
    "\n",
    "X, y = datasets.make_blobs(centers = n, n_features = 3, n_samples=n_samples, random_state=random_state, center_box=(-20.0,20.0))\n",
    "#transformation = [[1.5, -1, 1], [-1.5, 2, 1], [-1.5, 2, 1]]\n",
    "#X = np.dot(X, transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KernelParameters:\n",
    "    '''\n",
    "    A class to store the kernel parameters of all kernels in the fault network\n",
    "\n",
    "    mean: np.array\n",
    "        Array of shape (n_kernels, n_dimensions) that contains the mean for every kernel. \n",
    "\n",
    "    cov: np.array\n",
    "        Array of shape (n_kernels, n_dimensions, n_dimensions) that contains the covariance matrices for every kernel. \n",
    "    \n",
    "    weight: np.array\n",
    "        Array of shape (n_kernels,) that contains the weight of every kernel.\n",
    "\n",
    "    bbox: np.array\n",
    "        An array of shape (n_kernels,8,3) that contains the corners of the bounding box.\n",
    "    \n",
    "    is_bkg: np.array\n",
    "        A boolean array of shape(n_kernels,) that indicates which kernel is a background kernel\n",
    "    '''\n",
    "\n",
    "    def __init__(self, n_dim: int = 3):\n",
    "\n",
    "        self.n_kernels = 0\n",
    "        self.n_dim = n_dim\n",
    "        \n",
    "        self.mean = np.zeros((0,n_dim), dtype=np.float128)\n",
    "        self.cov = np.zeros((0,n_dim,n_dim), dtype=np.float128)\n",
    "        self.weight = np.zeros((0,), dtype=np.float128)\n",
    "        self.bbox = np.zeros((0,2**n_dim,n_dim), dtype=np.float128)\n",
    "        self.is_bkg = np.zeros((0,), dtype=bool)\n",
    "    \n",
    "    def get_dim(self):\n",
    "        return self.n_dim\n",
    "    \n",
    "    def get_n_kernels(self):\n",
    "        return self.n_kernels\n",
    "    \n",
    "    def add_kernels(self, mean, cov, weight, bbox, is_bkg):\n",
    "        '''Add new kernels to the kernel configuration'''\n",
    "\n",
    "        self.mean = np.concatenate([self.mean, mean], axis=0)\n",
    "        self.cov = np.concatenate([self.cov, cov], axis=0)\n",
    "        self.weight = np.concatenate([self.weight, weight], axis=0)\n",
    "        self.bbox = np.concatenate([self.bbox, bbox], axis=0)\n",
    "        self.is_bkg = np.concatenate([self.is_bkg, is_bkg], axis=0)\n",
    "\n",
    "        self.n_kernels += len(weight)\n",
    "\n",
    "    def concatenate_parameters(self, kernelp):\n",
    "        '''Add new kernels from another KernelParameters object'''\n",
    "\n",
    "        self.add_kernels(kernelp.mean, kernelp.cov, kernelp.weight, kernelp.bbox, kernelp.is_bkg)\n",
    "    \n",
    "    def modify_kernels(self, kernel_idx, mean, cov, weight, bbox, is_bkg):\n",
    "        '''\n",
    "        Modify a single or multiple existing kernels in the kernel configuration.\n",
    "        Make sure that the type of kernel_idx and the shape of the other arguments match.\n",
    "        '''\n",
    "\n",
    "        self.mean[kernel_idx] = mean\n",
    "        self.cov[kernel_idx] = cov\n",
    "        self.weight[kernel_idx] = weight\n",
    "        self.bbox[kernel_idx] = bbox\n",
    "        self.is_bkg[kernel_idx] = is_bkg\n",
    "    \n",
    "    def modify_weight(self, weight):\n",
    "        '''\n",
    "        Modify the weights of all kernels in the kernel configuration\n",
    "        '''\n",
    "\n",
    "        if self.weight.shape != weight.shape:\n",
    "            raise ValueError('Input weight matrix must have same shape as current weight matrix')\n",
    "\n",
    "        self.weight = weight\n",
    "\n",
    "    def get(self, field: str):\n",
    "        '''\n",
    "        Get a specific field from the kernel parameters.\n",
    "        '''\n",
    "\n",
    "        if field == 'm':\n",
    "            return self.mean\n",
    "        elif field == 'c':\n",
    "            return self.cov\n",
    "        elif field == 'w':\n",
    "            return self.weight\n",
    "        elif field == 'b':\n",
    "            return self.bbox\n",
    "        elif field == 'ib':\n",
    "            return self.is_bkg\n",
    "        else:\n",
    "            raise ValueError(f'Unknown field {field}')\n",
    "    \n",
    "    def delete_kernels(self, kernel_idx):\n",
    "        '''\n",
    "        Remove a single or multiple kernels from the kernel configuration.\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        kernel_idx: int | array_like\n",
    "            Integer, an integer index or a boolean mask of length n_kernels that indicates which kernels to keep\n",
    "\n",
    "        '''\n",
    "\n",
    "        if hasattr(kernel_idx, \"__len__\") and isinstance(kernel_idx[0], bool):\n",
    "            self.mean = self.mean[kernel_idx]\n",
    "            self.cov = self.cov[kernel_idx]\n",
    "            self.weight = self.weight[kernel_idx]\n",
    "            self.bbox = self.bbox[kernel_idx]\n",
    "            self.is_bkg = self.is_bkg[kernel_idx]\n",
    "\n",
    "            self.n_kernels = sum(kernel_idx)\n",
    "\n",
    "        else:\n",
    "            self.mean = np.delete(self.mean, kernel_idx, axis=0)\n",
    "            self.cov = np.delete(self.cov, kernel_idx, axis=0)\n",
    "            self.weight = np.delete(self.weight, kernel_idx, axis=0)\n",
    "            self.bbox = np.delete(self.bbox, kernel_idx, axis=0)\n",
    "            self.is_bkg = np.delete(self.is_bkg, kernel_idx, axis=0)\n",
    "\n",
    "            self.n_kernels -= len(kernel_idx) if hasattr(kernel_idx, \"__len__\") else 1\n",
    "    \n",
    "    def get_kernels(self, kernel_idx = None):\n",
    "        '''\n",
    "        Extract the kernel parameters of a single or multiple kernels.\n",
    "        If kernel_idx = None, return all kernels. \n",
    "        If kernel_idx is an integer, the outer dimension of the return values is flattened.\n",
    "        '''\n",
    "\n",
    "        if kernel_idx is None:\n",
    "            kernel_idx = range(self.n_kernels)\n",
    "\n",
    "        return self.mean[kernel_idx], self.cov[kernel_idx], self.weight[kernel_idx], self.bbox[kernel_idx], self.is_bkg[kernel_idx]\n",
    "    \n",
    "    def is_background(self, kernel_idx):\n",
    "\n",
    "        return self.is_bkg[kernel_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_capacity_clusters(X: np.array, \n",
    "                          min_sz_cluster: int = 4, \n",
    "                          min_n_merges: int = 4\n",
    "                          )->np.array:\n",
    "    '''\n",
    "    Get the cluster assignment with the largest number of valid clusters based on ward linkage.\n",
    "    Iteratively cuts the tree from the bottom until the number of valid clusters does not increase anymore.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    X: np.array\n",
    "        The data to compute the cluster assignment for as an array of size (n_samples x n_dimensions)\n",
    "    \n",
    "    min_sz_cluster: int\n",
    "        The threshold on the cluster size for a cluster to be considered valid, default = 4\n",
    "\n",
    "    min_n_merges: int\n",
    "        The number of cluster merging steps that can be skipped by the algorithm, default = 4\n",
    "\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    capacity_labels: np.array\n",
    "        The cluster labels as an array of length n_samples\n",
    "    '''\n",
    "\n",
    "    min_n_merges = max(min_sz_cluster, min_n_merges)\n",
    "\n",
    "    link_tree = ward(X)\n",
    "\n",
    "    capacity = 0\n",
    "    capacity_labels = np.zeros(len(X))\n",
    "\n",
    "    for n_merges in range(min_n_merges,len(X)):\n",
    "\n",
    "        # get cluster labels and determine the cluster sizes\n",
    "        labels = fcluster(link_tree, link_tree[n_merges-1, 2], \"distance\")-1\n",
    "        uq_labs , counts = np.unique(labels, return_counts = True)\n",
    "        \n",
    "        n_clusters = sum(counts >= min_sz_cluster)\n",
    "\n",
    "        # check whether capacity has improved\n",
    "        if n_clusters >= capacity:\n",
    "            capacity = n_clusters\n",
    "            capacity_labels = labels\n",
    "        \n",
    "        # stop when all points are included in valid clusters\n",
    "        elif n_clusters == len(uq_labs):\n",
    "            break\n",
    "\n",
    "\n",
    "    return capacity_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_gaussian_kernels(X: np.array,\n",
    "                         cluster_labels: np.array,\n",
    "                         min_sz_cluster: int = 4\n",
    "                         )->tuple:\n",
    "    '''\n",
    "    Fit a Gaussian kernel to every valid cluster. Fits mean, covariance and weight for every kernel.\n",
    "    If there are points that are not in any valid cluster, fit a uniform background kernel.\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    X: np.array\n",
    "        An array of observations. Must be of shape (n_samples, n_dimensions).\n",
    "    \n",
    "    cluster_labels: np.array\n",
    "        The cluster assignment of every observation. Must be of length n_samples\n",
    "    \n",
    "    min_sz_cluster: int\n",
    "        The threshold on the cluster size for a cluster to be considered valid, default = 4\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    kernels: KernelParameters\n",
    "        The kernel configuration after fitting Gaussian kernels to valid clusters\n",
    "\n",
    "    '''\n",
    "\n",
    "    if len(X) != len(cluster_labels):\n",
    "        raise ValueError(f'Number of datapoints {len(X)} does not match number of labels {len(cluster_labels)}')\n",
    "    \n",
    "    # determine dataset parameters\n",
    "    n_dim = X.shape[1]\n",
    "    n_points = X.shape[0]\n",
    "    uq_clusters, cluster_szs = np.unique(cluster_labels, return_counts = True) # all clusters and sizes\n",
    "    n_clusters = sum(cluster_szs >= min_sz_cluster) # number of valid clusters\n",
    "    valid_clusters = uq_clusters[cluster_szs >= min_sz_cluster] # labels of valid clusters\n",
    "    fit_background = n_clusters < len(uq_clusters)\n",
    "\n",
    "    mean = np.zeros((n_clusters+int(fit_background), n_dim))\n",
    "    covar = np.repeat([np.eye(n_dim)], n_clusters+int(fit_background), axis=0)\n",
    "    weight = np.zeros(n_clusters+int(fit_background))\n",
    "    bbox = np.all((n_clusters+int(fit_background), 8, 3), np.nan)\n",
    "    is_bkg = np.all(n_clusters+int(fit_background), False)\n",
    "\n",
    "    for i in range(n_clusters):\n",
    "\n",
    "        id = valid_clusters[i]\n",
    "        X_curr = X[cluster_labels == id] \n",
    "\n",
    "        # compute cluster mean\n",
    "        mean[i,:] = np.mean(X_curr, axis=0)\n",
    "\n",
    "        # compute cluster covariance\n",
    "        covar[i,:,:] = np.cov(X_curr, rowvar=False)\n",
    "\n",
    "        # compute cluster weight\n",
    "        weight[i] = len(X_curr)/n_points\n",
    "    \n",
    "    if fit_background and n_dim == 3:\n",
    "        # fit the background kernel\n",
    "\n",
    "        X_bkg = X[[l not in valid_clusters for l in cluster_labels]]\n",
    "        bbox[-1,:,:], center = get_minimum_bbox(X_bkg)\n",
    "        is_bkg[-1] = True\n",
    "\n",
    "        # set background mean\n",
    "        mean[-1,:] = center\n",
    "\n",
    "        # compute background covariance\n",
    "        #FIXME: sqrt(12)*stddev in paper but in the implementation it's sqrt(12)*variance?\n",
    "        covar[-1,:] = np.cov(bbox, rowvar=False)*3.5 \n",
    "\n",
    "        # compute background weight\n",
    "        weight[-1] = len(X_bkg)/n_points\n",
    "    \n",
    "    kernels = KernelParameters()\n",
    "    kernels.add_kernels(mean, covar, weight, bbox, is_bkg)\n",
    "\n",
    "\n",
    "    return kernels\n",
    "\n",
    "        \n",
    "\n",
    "def get_minimum_bbox(X: np.array)->tuple:\n",
    "\n",
    "    '''\n",
    "    Calculate the minimum volume oriented bounding box for the points in X\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    X: np.array\n",
    "        An array containing a point cloud of observations. Has to be of shape (n_samples, 3)\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    corners: np.array\n",
    "        The 8 corner points of the bounding box as an array of shape (8,3)\n",
    "\n",
    "    center: np.array\n",
    "        The center point of the bounding box as an array of shape (3,)\n",
    "    \n",
    "    '''\n",
    "\n",
    "    if X.shape[1] != 3:\n",
    "        raise ValueError(f'Data has to be 3-dimensional, was {X.shape[1]}-dimensional')\n",
    "\n",
    "    # create a point cloud object from the data\n",
    "    cloud = o3d.geometry.PointCloud()\n",
    "    cloud.points = o3d.utility.Vector3dVector(X)\n",
    "\n",
    "    # get the corners and center of the minimum bounding box\n",
    "    bbox = cloud.get_minimal_oriented_bounding_box()\n",
    "\n",
    "    corners = np.asarray(bbox.get_box_points())\n",
    "    center = bbox.get_center()\n",
    "\n",
    "    return corners, center\n",
    "\n",
    "\n",
    "def get_gaussian_bbox(mean: np.array,\n",
    "                      cov: np.array,\n",
    "                      n_var: float)->tuple:\n",
    "    '''\n",
    "    Calculate the minimum volume oriented bounding box for the points within sqrt(n_var) standard deviations of the mean of a 3D-Gaussian.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    mean: np.array\n",
    "        Mean vector of the Gaussian. Must be of shape (3,).\n",
    "    \n",
    "    cov: np.array\n",
    "        Covariance matrix of the Gaussian. Must be of shape (3,3).\n",
    "    \n",
    "    n_var: float\n",
    "        How many variances from the mean of the Gaussian to consider for the bounding box.\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "\n",
    "    corners: np.array\n",
    "        The 8 corner points of the bounding box as an array of shape (8,3)\n",
    "\n",
    "    mean: np.array\n",
    "        The center point of the bounding box as an array of shape (3,). Is equivalent to mean vector of the Gaussian.\n",
    "\n",
    "    '''\n",
    "\n",
    "    if mean.shape != (3,) or cov.shape != (3,3):\n",
    "        raise ValueError('Gaussian must be 3-dimensional')\n",
    "    \n",
    "    evals, evecs = np.sqrt(np.linalg.eigh(n_var*cov))\n",
    "\n",
    "    # determine lengths of the bbox edges\n",
    "    l_sides = 0.5*np.sqrt(evals) \n",
    "    \n",
    "    # get an unrotated cuboid centered around 0 of the correct size\n",
    "    corners = np.array([[-1, 1, 1, -1, -1, 1, 1, -1],\n",
    "                        [1, 1, 1, 1, -1, -1, -1, -1],\n",
    "                        [-1, -1, 1, 1, 1, 1, -1, -1]])\n",
    "    \n",
    "    corners *= np.expand_dims(l_sides, axis=1)\n",
    "    \n",
    "    # rotate and shift the cuboid to the correct position and get the correct shape\n",
    "    corners = (evecs @ corners).T + np.tile(mean, [8,1])\n",
    "\n",
    "    return corners, mean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def inhull(X: np.array,\n",
    "           hull_pts: np.array,\n",
    "           eps: float = np.finfo(float).eps\n",
    "          )->np.array:\n",
    "  \n",
    "  '''\n",
    "  Check for all points in X whether they lie in the convex hull defined by hull_pts.\n",
    "  Adapted from https://stackoverflow.com/questions/31404658/check-if-points-lies-inside-a-convex-hull\n",
    "\n",
    "  Parameters\n",
    "  -----------\n",
    "  X: np.array\n",
    "    An array of observations. Must be of shape (n_samples, n_dimensions).\n",
    "  \n",
    "  hull_pts: np.array\n",
    "    An array of points from which the convex hull is determined. Must be of shape (n_points, n_dimensions).\n",
    "  \n",
    "  eps: np.float32\n",
    "    The tolerance to be used when checking whether a given point is inside the hull.\n",
    "    Choose > 0 to avoid numerical issues, default = np.finfo(float).eps\n",
    "\n",
    "  Returns\n",
    "  --------\n",
    "  in_hull: np.array\n",
    "    A boolean array of shape (n_samples,) indicating which points in X are inside the hull\n",
    "    \n",
    "  '''\n",
    "\n",
    "  if X.shape[1] != hull_pts.shape[1]:\n",
    "    raise ValueError(f'Hull points and test points must have the same dimensions.')\n",
    "  \n",
    "  hull = ConvexHull(hull_pts)\n",
    "\n",
    "  # A is shape (f, d) and b is shape (f, 1).\n",
    "  A, b = hull.equations[:, :-1], hull.equations[:, -1]\n",
    "\n",
    "  # The hull is defined as all points x for which Ax + b <= 0.\n",
    "  in_hull = np.array([np.all(A @ x + b < eps) for x in X])\n",
    "\n",
    "  return in_hull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_to_kernel(X: np.array,\n",
    "                     kernels: KernelParameters,\n",
    "                     min_sz_cluster: int = 4\n",
    "                     ):\n",
    "    '''\n",
    "    Perform a single step of expectation maximization to assign each data point to its kernel.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.ndarray\n",
    "        An array of observations. Must be of shape (n_samples, n_dimensions).\n",
    "\n",
    "    kernels: KernelParameters\n",
    "        The current kernel configuration\n",
    "    \n",
    "    min_sz_cluster: int\n",
    "        The threshold on the cluster size for a cluster to be considered valid, default = 4\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    kernels: KernelParameters\n",
    "        The updated kernel configuration\n",
    "    \n",
    "    k_assign: np.ndarray\n",
    "        An array of shape (n_samples,) which indicates the kernel assignment of every point in X\n",
    "\n",
    "    kernel_prob: np.ndarray\n",
    "        An array of shape (n_samples, n_kernels) that contains the probability of each datapoint under each kernel\n",
    "    '''\n",
    "\n",
    "    n_points = len(X)\n",
    "\n",
    "    kernel_prob = get_kernel_prob(X, kernels)\n",
    "    k_assign = np.argmax(kernel_prob, axis=1)\n",
    "\n",
    "    del_ker = []\n",
    "\n",
    "    for i in range(kernels.get_n_kernels()):\n",
    "\n",
    "        msk = k_assign == i\n",
    "        cluster_sz = sum(msk)\n",
    "\n",
    "        if cluster_sz >= min_sz_cluster or (kernels.is_background(i) and cluster_sz > 0):\n",
    "            \n",
    "            # update kernel parameters\n",
    "            cov = np.cov(X[msk,:], rowvar=False)\n",
    "            mean = np.mean(X[msk,:], axis=0)\n",
    "            weight = cluster_sz / n_points\n",
    "\n",
    "            if not kernels.is_background(i):\n",
    "                # calculate bbox for every non-background kernel\n",
    "\n",
    "                # bbox defined by the points in the cluster\n",
    "                bbox_pts, _ = get_minimum_bbox(X[msk,:])\n",
    "\n",
    "                # bbox defined by the Gaussian kernel\n",
    "                bbox_gauss, _ = get_gaussian_bbox(mean[i,:], cov[i,:,:], 12)\n",
    "\n",
    "                # bbox is minimum bbox of the union of all corner points\n",
    "                bbox, _ = get_minimum_bbox(np.concatenate((bbox_pts, bbox_gauss), axis=0))\n",
    "            \n",
    "            kernels.modify_kernels(i, mean, cov, weight, bbox, kernels.is_background(i))\n",
    "\n",
    "        else:\n",
    "    \n",
    "            del_ker.append[i]\n",
    "\n",
    "    # delete non-background kernels with too few points\n",
    "    kernels.delete_kernels(del_ker)\n",
    "\n",
    "    # update probabilities and kernel assignment\n",
    "    kernel_prob = get_kernel_prob(X, kernels)\n",
    "    k_assign = np.argmax(kernel_prob, axis=1)\n",
    "\n",
    "    return kernels, k_assign, kernel_prob\n",
    "\n",
    "\n",
    "\n",
    "def get_kernel_prob(X: np.array,\n",
    "                     kernels: KernelParameters\n",
    "                     ):\n",
    "    '''\n",
    "    Determine the probability of each data point under each kernel\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.ndarray\n",
    "        An array of observations. Must be of shape (n_samples, n_dimensions).\n",
    "\n",
    "    kernels: KernelParameters\n",
    "        The current kernel configuration\n",
    "        \n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    kernel_prob: np.array\n",
    "        An array of shape (n_samples, n_kernels)\n",
    "    '''\n",
    "\n",
    "    kernel_prob = np.zeros((len(X), kernels.get_n_kernels()), dtype=np.float128)\n",
    "\n",
    "    # calculate the the probability of each point under each kernel\n",
    "    for i in range(kernels.get_n_kernels()):\n",
    "\n",
    "        mean, cov, weight, bbox, is_bkg = kernels.get_kernels(i)\n",
    "\n",
    "        if not is_bkg:\n",
    "\n",
    "            kernel_prob[:,i] = weight[i] * multivariate_normal(X, mean=mean, cov=cov)\n",
    "    \n",
    "        else:\n",
    "            # determine points in background kernel\n",
    "            eps = 1.e^-10*np.mean(np.abs(bbox))\n",
    "            bkg_pts = inhull(X, bbox, eps)\n",
    "            \n",
    "            # calculate probability of being in the background\n",
    "            kernel_prob[bkg_pts,i] = weight*1/np.prod(np.sqrt(np.linalg.eigvalsh(cov)), axis=0)\n",
    "\n",
    "    return kernel_prob\n",
    "\n",
    "\n",
    "def get_bic(kernel_prob: np.array,\n",
    "            n_kernels: int = None):\n",
    "    '''\n",
    "    Calculate the Bayesian Information Criterion of the dataset from the probability of each datapoint under each kernel\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    kernel_prob: np.array\n",
    "        An array of shape (n_samples, n_kernels) that contains the probability of every sample under every kernel\n",
    "    \n",
    "    n_kernels: int\n",
    "        Optional number of kernels to use. Can be specified if kernel_prob is already a cumulative probability, default = None\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    bic: float\n",
    "        The BIC value of the current kernel configuration\n",
    "    '''\n",
    "    \n",
    "    n_points = kernel_prob.shape[0]\n",
    "    \n",
    "    if n_kernels is None:\n",
    "        n_kernels = kernel_prob.shape[1]\n",
    "\n",
    "    # get cumulative probability per data point\n",
    "    total_prob = np.sum(kernel_prob, axis = 0)\n",
    "    total_prob[total_prob < np.finfo(np.float128).eps] = np.finfo(np.float128).eps\n",
    "\n",
    "    # calculate BIC\n",
    "    bic = np.sum(-np.log(total_prob))+0.5*(10*n_kernels-1)*np.log(n_points)\n",
    "\n",
    "    return bic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def have_overlap(bbox1: np.array,\n",
    "                 bbox2: np.array):\n",
    "    '''\n",
    "    Check whether two bounding boxes overlap.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    bbox1: np.array\n",
    "        The corner points of the first bounding box as an array of shape (n_points, n_dimensions)\n",
    "    \n",
    "    bbox1: np.array\n",
    "        The corner points of the second bounding box as an array of shape (n_points, n_dimensions)\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    overlap: bool\n",
    "        Indicates whether the two bounding boxes overlap\n",
    "\n",
    "    '''\n",
    "    \n",
    "    overlap = inhull(bbox1, bbox2) or inhull(bbox2, bbox1)\n",
    "\n",
    "    return overlap\n",
    "\n",
    "\n",
    "\n",
    "def merge_single_pair(kernel_pair: KernelParameters,\n",
    "                      keep_wgt: bool = True):\n",
    "    '''\n",
    "    Perform a Gaussian merge on a pair of kernels\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    kernel_pair: KernelParameters\n",
    "        Contains the two kernels for which the merged kernel should be computed\n",
    "    \n",
    "    keep_wgt: bool\n",
    "        Indicates whether to compute the weight of the merged kernel or set it to 1, default = True\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    mean: np.array\n",
    "        The mean vector of the merged kernel, has shape (1, n_dim)\n",
    "    \n",
    "    cov: np.array\n",
    "        The covariance matrix of the merged kernel, has shape (1, n_dim, n_dim)\n",
    "    \n",
    "    weight: np.array\n",
    "        The weight of the merged kernel, has shape (1,)\n",
    "    \n",
    "    bbox: np.array\n",
    "        The bounding box of the merged kernel, has shape(1, 2**n_dim, n_dim)\n",
    "    \n",
    "    is_bkg: np.array\n",
    "        Indicates whether the merged kernel is a background kernel, has shape (1,)\n",
    "\n",
    "    '''\n",
    "    if kernel_pair.get_n_kernels() != 2:\n",
    "        raise ValueError('Kernel configuration is not a pair')\n",
    "\n",
    "    # unpack the kernel parameters\n",
    "    m1, c1, w1, b1, ib1 = kernel_pair.get_kernels(0)\n",
    "    m2, c2, w2, b2, ib2 = kernel_pair.get_kernels(1)\n",
    "    \n",
    "    # calculate merged bbox and weight\n",
    "    weight = w1+w2\n",
    "    bbox, center = get_minimum_bbox(np.concatenate((b1,b2), axis=0))\n",
    "    is_bkg = ib1 or ib2\n",
    "\n",
    "    # calculate new mean and covariance\n",
    "    if is_bkg:\n",
    "        mean = center\n",
    "        cov = np.cov(bbox, rowvar=False)*3.5\n",
    "    \n",
    "    else:\n",
    "        mean = 1/weight * (w1 * m1 + w2 * m2)\n",
    "        cov = (w1/weight)*(c1 + np.outer((m1-mean),(m1-mean))) + (w2/weight)*(c2 + np.outer((m2-mean),(m2-mean)))\n",
    "\n",
    "\n",
    "    if not keep_wgt:\n",
    "        weight = 1\n",
    "\n",
    "    return np.array([mean]), np.array([cov]), np.array([weight]), np.array([bbox]), np.array([is_bkg])\n",
    "\n",
    "\n",
    "\n",
    "def get_disjoint_pairs(rows: np.array,\n",
    "                       cols: np.array,\n",
    "                       score: np.array):\n",
    "    '''\n",
    "    Get the disjoint pairs from a set of row-column index pairs.\n",
    "    The first occurrence of an index is decided based on descending score.\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    rows: np.array\n",
    "        An array of row indices\n",
    "    \n",
    "    cols: np.array\n",
    "        An array of column indices in the same order as rows\n",
    "    \n",
    "    score: np.array\n",
    "        An array of scores that determine the precedence of a pair in the same order as rows.\n",
    "        Higher score is better and pairs with non-positive scores are cut.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    rows, columns, score: np.array\n",
    "        The row and column indices and scores of the unique pairs sorted by descending score\n",
    "    \n",
    "    idx_sort: np.array\n",
    "        An index that can be used to extract the matching elements of an array with the same order\n",
    "        as the input rows or columns\n",
    "    '''\n",
    "\n",
    "    # get all pairs with improvement\n",
    "    idx_imp = score > 0\n",
    "\n",
    "    rows = rows[idx_imp]\n",
    "    cols = cols[idx_imp]\n",
    "    score = np.array(score)[idx_imp]\n",
    "    \n",
    "    # sort the pairs by descending score\n",
    "    idx_sort = np.argsort(score)[::-1]\n",
    "    rows = rows[idx_sort]\n",
    "    cols = cols[idx_sort]\n",
    "\n",
    "     \n",
    "    unique_rows = []\n",
    "    unique_cols = []\n",
    "\n",
    "    # only keep disjoint pairs\n",
    "    for i in range(len(score)):\n",
    "        if (rows[i] in unique_rows or unique_cols) or (cols[i] in unique_cols or unique_rows):\n",
    "            score = np.delete(score, i)\n",
    "            idx_sort = np.delete(idx_sort, i)\n",
    "        else:\n",
    "            unique_rows.append(rows[i])\n",
    "            unique_cols.append(cols[i])\n",
    "\n",
    "    return np.array(unique_rows), np.array(unique_cols), score, idx_sort\n",
    "\n",
    "\n",
    "def merge_kernel_assignment(kernel_assign: np.ndarray,\n",
    "                            kernel1: np.ndarray, \n",
    "                            kernel2: np.ndarray):\n",
    "    \n",
    "    '''\n",
    "    Map the old kernel labels to the new kernel labels.\n",
    "    Kernel labels coincide with position of the kernel in the KernelParameters object.\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    kernel_assign: np.ndarray\n",
    "        The old kernel assignment of every data point. Has shape (n_samples,)\n",
    "    \n",
    "    kernel1, kernel2: np.ndarray\n",
    "        Contain the labels of the first and second kernel of every merged pair in the same order\n",
    "\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    kernel_assign: np.ndarray\n",
    "        The new kernel assignment of ever data point\n",
    "\n",
    "    '''\n",
    "\n",
    "    # create a mapping from old kernel labels to new kernel labels\n",
    "    old2new = np.arange(len(np.unique(kernel_assign)))\n",
    "    \n",
    "    msk_keep = np.full(len(old2new)+1, True)\n",
    "    msk_keep[np.concatenate([kernel1, kernel2])] = False\n",
    "\n",
    "    # add new labels of the old kernels \n",
    "    old2new = old2new[msk_keep]\n",
    "    old2new = dict(zip(old2new, range(len(old2new))))\n",
    "\n",
    "    # add new labels of the merged kernels\n",
    "    for i in range(len(kernel1)):\n",
    "\n",
    "        old2new[kernel1[i]] = len(old2new)+i\n",
    "        old2new[kernel2[i]] = len(old2new)+i\n",
    "\n",
    "\n",
    "    kernel_assign = np.array([old2new[l] for l in kernel_assign])\n",
    "\n",
    "    return kernel_assign\n",
    "\n",
    "\n",
    "def merge_clusters(X: np.ndarray,\n",
    "                   kernels: KernelParameters,\n",
    "                   kernel_assign: np.ndarray,\n",
    "                   init_prob: np.ndarray,\n",
    "                   gain_mode: str = 'global'):\n",
    "    \n",
    "    '''\n",
    "    Merge clusters iteratively \n",
    "    '''\n",
    "    \n",
    "    modes = ['local', 'global']\n",
    "\n",
    "    if gain_mode not in modes:\n",
    "        raise ValueError(f'Unknown gain mode {gain_mode}. Must be one of {modes}.')\n",
    "\n",
    "    bic_init = get_bic(init_prob)\n",
    "    tot_prob_init = np.sum(init_prob, axis=0, keepdims=True)\n",
    "\n",
    "    gain = np.zeros((kernels.get_n_kernels(),kernels.get_n_kernels()), dtype=np.float128)\n",
    "\n",
    "    while True:\n",
    "\n",
    "        # get indices of all relevant kernel pairs\n",
    "        rows,cols = np.mask_indices(len(gain), lambda x,k: np.logical_and(np.triu(x,k),~np.isnan(gain)),1)\n",
    "        \n",
    "        # check whether pairs are merging candidates\n",
    "        del_idx = np.full((len(rows)), False)\n",
    "        for idx, (r, c) in enumerate(zip(rows,cols)):\n",
    "\n",
    "            _, __, ___, r_bbox, r_bkg = kernels.get_kernels(r)\n",
    "            _, __, ___, c_bbox, c_bkg = kernels.get_kernels(c)\n",
    "\n",
    "            del_idx[idx] = r_bkg or c_bkg\n",
    "\n",
    "            if not del_idx[idx]:\n",
    "                del_idx[idx] = not have_overlap(r_bbox, c_bbox)\n",
    "        \n",
    "        # remove all non-candidates\n",
    "        gain[rows[del_idx],cols[del_idx]] = np.nan\n",
    "        gain[cols[del_idx],rows[del_idx]] = np.nan\n",
    "\n",
    "        rows = rows[~del_idx]\n",
    "        cols = cols[~del_idx]\n",
    "        \n",
    "        if len(rows)==0:\n",
    "            break\n",
    "\n",
    "\n",
    "        new_kernels = KernelParameters()\n",
    "        p_merged = []\n",
    "        p_separate = []\n",
    "        merge_score = []\n",
    "\n",
    "        for idx, (r, c) in enumerate(zip(rows,cols)):\n",
    "\n",
    "            # get the pair kernels\n",
    "            old_kerns = KernelParameters()\n",
    "            old_kerns.add_kernels(kernels.get_kernels([r,c]))\n",
    "\n",
    "            new_kern = KernelParameters()\n",
    "                \n",
    "            if gain_mode == 'local':\n",
    "                \n",
    "                # only consider contributions from points assigned to the kernel pair\n",
    "                X_local = X[np.logical_or(kernel_assign == r, kernel_assign == c)]\n",
    "\n",
    "                # calculate parameters of the merged kernel\n",
    "                new_kern.add_kernels(merge_single_pair(old_kerns, keep_w = False))\n",
    "\n",
    "                # get probability and bic under the merged kernel\n",
    "                prob_merged = get_kernel_prob(X_local, new_kern)\n",
    "                bic_merged = get_bic(prob_merged)\n",
    "\n",
    "                # modify weight\n",
    "                old_kerns.modify_weight(np.array([sum(kernel_assign==r),sum(kernel_assign==c)], dtype=np.float128)/len(X_local))\n",
    "\n",
    "                # get probability and bic under the two separate kernels\n",
    "                prob_separate = get_kernel_prob(X_local, old_kerns)\n",
    "                bic_separate = get_bic(prob_separate)\n",
    "                \n",
    "                # calculate the information gain from merging\n",
    "                merge_score.append(bic_separate-bic_merged)\n",
    "        \n",
    "        \n",
    "            else:\n",
    "                # calculate parameters of the merged kernel\n",
    "                new_kern.add_kernels(merge_single_pair(old_kerns, keep_w = True))\n",
    "\n",
    "                # get cumulative probability under the merged kernel\n",
    "                tot_prob_merged = np.sum(get_kernel_prob(X, new_kern), axis=0, keepdims=True)\n",
    "                p_merged.append(tot_prob_merged)\n",
    "\n",
    "                # get cumulative probability under the two separate kernels\n",
    "                tot_prob_separate = np.sum(get_kernel_prob(X, old_kerns), axis=0, keepdims=True)\n",
    "                p_separate.append(tot_prob_separate)\n",
    "                \n",
    "                # calculate the cumulative probability after merging and bic\n",
    "                sum_tot_prob = tot_prob_init - tot_prob_separate + tot_prob_merged\n",
    "                bic_merged = get_bic(sum_tot_prob, n_kernels = kernels.get_n_kernels()-1)\n",
    "\n",
    "                # calculate the information gain from merging\n",
    "                merge_score.append(bic_init-bic_merged)\n",
    "\n",
    "                # save the new kernel\n",
    "                new_kernels.concatenate_parameters(new_kern)\n",
    "        \n",
    "        gain[rows, cols] = merge_score\n",
    "        gain[cols, rows] = merge_score\n",
    "\n",
    "        if np.nanmax(merge_score) <= 0:\n",
    "            \n",
    "            if gain_mode == 'local':\n",
    "            \n",
    "                # switch to global optimization after local is finished\n",
    "                gain_mode = 'global'\n",
    "                continue\n",
    "        \n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        msk_sort = np.full(len(rows), False)\n",
    "\n",
    "        # get unique pairs\n",
    "        rows, cols, merge_score, idx_sort = get_disjoint_pairs(rows, cols, merge_score)\n",
    "        n_pairs = len(rows)\n",
    "\n",
    "        # merge unique pairs\n",
    "        if gain_mode == 'local':\n",
    "\n",
    "            # merge clusters with global method\n",
    "            p_merged = np.zeros(n_pairs)\n",
    "            p_separate = np.zeros(n_pairs)\n",
    "            \n",
    "            for r,c in zip(rows, cols):\n",
    "\n",
    "                # get the pair kernels\n",
    "                old_kerns = KernelParameters()\n",
    "                old_kerns.add_kernels(kernels.get_kernels([r,c]))\n",
    "                \n",
    "                # calculate parameters of the merged kernel\n",
    "                new_kern = KernelParameters()\n",
    "                new_kern.add_kernels(merge_single_pair(old_kerns, keep_w = True))\n",
    "\n",
    "                # get cumulative probability under the merged kernel\n",
    "                p_merged += np.sum(get_kernel_prob(X, new_kern), axis=0, keepdims=True)\n",
    "\n",
    "                # get cumulative probability under the two separate kernels\n",
    "                p_separate += np.sum(get_kernel_prob(X, old_kerns), axis=0, keepdims=True)\n",
    "                \n",
    "                new_kernels.concatenate_parameters(new_kern)\n",
    "            \n",
    "        else:\n",
    "\n",
    "            # transform sort index to boolean mask to use on KernelParameters\n",
    "            msk_sort[idx_sort] = True\n",
    "\n",
    "            # can reuse results from above\n",
    "            new_kernels = new_kernels.delete_kernels(msk_sort)\n",
    "            p_merged = sum(p_merged[msk_sort])\n",
    "            p_separate = sum(p_separate[msk_sort])\n",
    "\n",
    "        # update the initial probability and get the bic of all the merging events\n",
    "        tot_prob_init = tot_prob_init - p_separate + p_merged\n",
    "        bic_init = get_bic(tot_prob_init, n_kernels=kernels.get_n_kernels()-new_kernels.get_n_kernels())\n",
    "\n",
    "        if np.isinf(bic_init):\n",
    "            break\n",
    "\n",
    "\n",
    "        # delete the now merged kernels\n",
    "        idx_del = np.concatenate([rows, cols], axis=0)\n",
    "\n",
    "        kernels.delete_kernels(idx_del)\n",
    "        gain = np.delete(gain, idx_del, axis=0)\n",
    "        gain = np.delete(gain, idx_del, axis=1)\n",
    "\n",
    "        # add the new kernels\n",
    "        kernels.concatenate_parameters(new_kernels)\n",
    "\n",
    "        # modify kernel assignment\n",
    "        if gain_mode == 'local':\n",
    "            kernel_assign = merge_kernel_assignment(kernel_assign, rows, cols)\n",
    "            \n",
    "        gain = np.zeros((kernels.get_n_kernels(),kernels.get_n_kernels()), dtype=np.float128)\n",
    "    \n",
    "    return kernels, kernel_assign\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_chunks(X: np.ndarray,\n",
    "               n_chunks: int):\n",
    "    '''\n",
    "    Cut X into n_chunks chunks based on its agglomerative tree\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.ndarray\n",
    "        An array of observations. Must be of shape (n_samples, n_dimensions).\n",
    "    \n",
    "    n_chunks: int\n",
    "        The number of chunks that the data should be cut into\n",
    "\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    chunk_labs: np.ndarray\n",
    "        An array of shape (n_samples,) that contains the chunk assignment of each datapoint\n",
    "    '''\n",
    "\n",
    "    if n_chunks == 1:\n",
    "        chunk_labs = np.zeros(len(X))\n",
    "    \n",
    "    else:\n",
    "        # compute the labels from the agglomerative link tree\n",
    "        link_tree = ward(X)\n",
    "        chunk_labs = fcluster(link_tree, link_tree[-n_chunks, 2], \"distance\")-1\n",
    "\n",
    "    return chunk_labs\n",
    "\n",
    "\n",
    "\n",
    "def run_fault_reconstruction(X: np.ndarray,\n",
    "                       min_sz_cluster: int,\n",
    "                       n_chunks: int = 1,\n",
    "                       gain_mode: str = 'global'\n",
    "                       ):\n",
    "    '''\n",
    "    Run the fault reconstruction algorithm (Kamer 2020)\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    X: np.ndarray\n",
    "        The data to compute the cluster assignment for as an array of size (n_samples x n_dimensions)\n",
    "    \n",
    "    min_sz_cluster: int\n",
    "        The threshold on the cluster size for a cluster to be considered valid, default = 4\n",
    "\n",
    "    n_chunks: int\n",
    "        Number of chunks that the data is cut into before performing the algorithm, default = 1\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    all_labels: np.ndarray\n",
    "        The cluster assignment of every data point\n",
    "    '''\n",
    "\n",
    "    # get the chunk partition\n",
    "    chunk_labs = cut_chunks(X, n_chunks)\n",
    "\n",
    "    all_kernels = KernelParameters()\n",
    "    all_labels = np.empty(0, dtype=int)\n",
    "    lab_offset = 0\n",
    "\n",
    "    for chunk_id in range(n_chunks):\n",
    "\n",
    "        msk = chunk_labs == chunk_id\n",
    "\n",
    "        # get the capacity clusters based on the agglomerative tree\n",
    "        capacity_labs = get_capacity_clusters(X[msk], min_sz_cluster)\n",
    "        \n",
    "        # fit the kernels\n",
    "        kernels = fit_gaussian_kernels(X[msk], capacity_labs, min_sz_cluster)\n",
    "\n",
    "        # assign points and update kernels with EM\n",
    "        kernels, cluster_labs, kernel_prob = assign_to_kernel(X[msk], kernels, min_sz_cluster)\n",
    "\n",
    "        # run the kernel merging algorithm\n",
    "        kernels, cluster_labs = merge_clusters(X[msk], kernels, cluster_labs, kernel_prob, gain_mode)\n",
    "\n",
    "        # reassign the points with EM\n",
    "        kernels, cluster_labs, kernel_prob = assign_to_kernel(X[msk], kernels, min_sz_cluster)\n",
    "\n",
    "        # shift and persist the cluster labels of the current chunk\n",
    "        cluster_labs = cluster_labs + lab_offset\n",
    "        lab_offset += kernels.get_n_kernels()\n",
    "        all_labels = np.concatenate([all_labels, cluster_labs], axis=0)\n",
    "\n",
    "        # scale the kernel weights\n",
    "        _, cluster_sz = np.unique(np.sort(cluster_labs), return_counts=True)\n",
    "        kernels.modify_weight(kernels.get('w')*cluster_sz/sum(msk))\n",
    "\n",
    "        # persist the kernels of the current chunk\n",
    "        all_kernels.concatenate_parameters(kernels)\n",
    "    \n",
    "\n",
    "    # merge the kernels of all chunks\n",
    "    kernel_prob = get_kernel_prob(X, all_kernels)\n",
    "    all_kernels, all_labels = merge_clusters(X, all_kernels, all_labels, kernel_prob, gain_mode)\n",
    "\n",
    "    return all_labels\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
