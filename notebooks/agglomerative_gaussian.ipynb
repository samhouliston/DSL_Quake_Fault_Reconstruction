{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import scipy as sc \n",
    "from sklearn import datasets\n",
    "from scipy.cluster.hierarchy import ward, fcluster\n",
    "import open3d as o3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state=71"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a simple dataset\n",
    "\n",
    "n_samples = 500\n",
    "n = 4\n",
    "\n",
    "X, y = datasets.make_blobs(centers = n, n_features = 3, n_samples=n_samples, random_state=random_state, center_box=(-20.0,20.0))\n",
    "#transformation = [[1.5, -1, 1], [-1.5, 2, 1], [-1.5, 2, 1]]\n",
    "#X = np.dot(X, transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_capacity_clusters(X: np.array, \n",
    "                          min_sz_cluster: int = 4, \n",
    "                          min_n_merges: int = 4\n",
    "                          )->np.array:\n",
    "    '''\n",
    "    Get the cluster assignment with the largest number of valid clusters based on ward linkage.\n",
    "    Iteratively cuts the tree from the bottom until the number of valid clusters does not increase anymore.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    X: np.array\n",
    "        The data to compute the cluster assignment for as an array of size (n_samples x n_dimensions)\n",
    "    \n",
    "    min_sz_cluster: int\n",
    "        The threshold on the cluster size for a cluster to be considered valid, default = 4\n",
    "\n",
    "    min_n_merges: int\n",
    "        The number of cluster merging steps that can be skipped by the algorithm, default = 4\n",
    "\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    capacity_labels: np.array\n",
    "        The cluster labels as an array of length n_samples\n",
    "    '''\n",
    "\n",
    "    min_n_merges = max(min_sz_cluster, min_n_merges)\n",
    "\n",
    "    link_tree = ward(X)\n",
    "\n",
    "    capacity = 0\n",
    "    capacity_labels = np.zeros(len(X))\n",
    "\n",
    "    for n_merges in range(min_n_merges,len(X)):\n",
    "\n",
    "        # get cluster labels and determine the cluster sizes\n",
    "        labels = fcluster(link_tree, link_tree[n_merges-1, 2], \"distance\")\n",
    "        uq_labs , counts = np.unique(labels, return_counts = True)\n",
    "        \n",
    "        n_clusters = sum(counts >= min_sz_cluster)\n",
    "\n",
    "        # check whether capacity has improved\n",
    "        if n_clusters >= capacity:\n",
    "            capacity = n_clusters\n",
    "            capacity_labels = labels\n",
    "        \n",
    "        # stop when all points are included in valid clusters\n",
    "        elif n_clusters == len(uq_labs):\n",
    "            break\n",
    "\n",
    "\n",
    "    return capacity_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_gaussian_kernels(X: np.array,\n",
    "                         cluster_labels: np.array,\n",
    "                         min_sz_cluster: int = 4\n",
    "                         )->tuple:\n",
    "    '''\n",
    "    Fit a Gaussian kernel to every valid cluster. Fits mean, covariance and weight for every kernel.\n",
    "    If there are points that are not in any valid cluster, fit a uniform background kernel.\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    X: np.array\n",
    "        An array of observations. Must be of shape (n_samples, n_dimensions).\n",
    "    \n",
    "    cluster_labels: np.array\n",
    "        The cluster assignment of every observation. Must be of length n_samples\n",
    "    \n",
    "    min_sz_cluster: int\n",
    "        The threshold on the cluster size for a cluster to be considered valid, default = 4\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mean: np.array\n",
    "        Array of shape (n_kernels, n_dimensions) that contains the covariance matrices for every kernel. \n",
    "        The last entry corresponds to the background kernel if there exist background points.\n",
    "\n",
    "    covar: np.array\n",
    "        Array of shape (n_kernels, n_dimensions, n_dimensions) that contains the covariance matrices for every kernel. \n",
    "        The last entry corresponds to the background kernel if there exist background points.\n",
    "    \n",
    "    weight: np.array\n",
    "        Array of shape (n_kernels,) that contains the weight of every kernel.\n",
    "        The last entry corresponds to the background kernel if there exist background points.\n",
    "\n",
    "    bbox: None | np.array\n",
    "        An array of shape (8,3) that contains the corners of the bounding box for the background kernel.\n",
    "        None if there are no background points.\n",
    "\n",
    "    '''\n",
    "\n",
    "    if len(X) != len(cluster_labels):\n",
    "        raise ValueError(f'Number of datapoints {len(X)} does not match number of labels {len(cluster_labels)}')\n",
    "    \n",
    "    # determine dataset parameters\n",
    "    n_dim = X.shape[1]\n",
    "    n_points = X.shape[0]\n",
    "    uq_clusters, cluster_szs = np.unique(cluster_labels, return_counts = True) # all clusters and sizes\n",
    "    n_clusters = sum(cluster_szs >= min_sz_cluster) # number of valid clusters\n",
    "    valid_clusters = uq_clusters[cluster_szs >= min_sz_cluster] # labels of valid clusters\n",
    "    fit_background = n_clusters < len(uq_clusters)\n",
    "\n",
    "    mean = np.zeros((n_clusters+int(fit_background), n_dim))\n",
    "    covar = np.repeat([np.eye(n_dim)], n_clusters+int(fit_background), axis=0)\n",
    "    weight = np.zeros(n_clusters+int(fit_background))\n",
    "    bbox = None\n",
    "\n",
    "    for i in range(n_clusters):\n",
    "\n",
    "        id = valid_clusters[i]\n",
    "        X_curr = X[cluster_labels == id] \n",
    "\n",
    "        # compute cluster mean\n",
    "        mean[i,:] = np.mean(X_curr, axis=0)\n",
    "\n",
    "        # compute cluster covariance\n",
    "        covar[i,:,:] = np.cov(X_curr, rowvar=False)\n",
    "\n",
    "        # compute cluster weight\n",
    "        weight[i] = len(X_curr)/n_points\n",
    "    \n",
    "    if fit_background and n_dim == 3:\n",
    "        # fit the background kernel\n",
    "\n",
    "        X_bkg = X[[l not in valid_clusters for l in cluster_labels]]\n",
    "        bbox, center = get_minimum_bbox(X_bkg)\n",
    "\n",
    "        # set background mean\n",
    "        mean[-1,:] = center\n",
    "\n",
    "        # compute background covariance\n",
    "        #FIXME: sqrt(12)*stddev in paper but in the implementation it's sqrt(12)*variance?\n",
    "        covar[-1,:] = np.cov(bbox, rowvar=False)*3.5 \n",
    "\n",
    "        # compute background weight\n",
    "        weight[-1] = len(X_bkg)/n_points\n",
    "    \n",
    "\n",
    "    return mean, covar, weight, bbox\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "def get_minimum_bbox(X: np.array)->tuple:\n",
    "\n",
    "    '''\n",
    "    Calculate the minimum volume oriented bounding box for the points in X\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    X: np.array\n",
    "        An array containing a point cloud of observations. Has to be of shape (n_samples, 3)\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    corners: np.array\n",
    "        The 8 corner points of the bounding box as an array of shape (8,3)\n",
    "\n",
    "    center: np.array\n",
    "        The center point of the bounding box as an array of shape (3,)\n",
    "    \n",
    "    '''\n",
    "\n",
    "    if X.shape[1] != 3:\n",
    "        raise ValueError(f'Data has to be 3-dimensional, was {X.shape[1]}-dimensional')\n",
    "\n",
    "    # create a point cloud object from the data\n",
    "    cloud = o3d.geometry.PointCloud()\n",
    "    cloud.points = o3d.utility.Vector3dVector(X)\n",
    "\n",
    "    # get the corners and center of the minimum bounding box\n",
    "    bbox = cloud.get_minimal_oriented_bounding_box()\n",
    "\n",
    "    corners = np.asarray(bbox.get_box_points())\n",
    "    center = bbox.get_center()\n",
    "\n",
    "    return corners, center\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = get_capacity_clusters(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
